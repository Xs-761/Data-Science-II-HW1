---
title: "Data Science II, HW1"
output: github_document
---

```{r packages, warnig=FALSE}
  library(glmnet)
  library(caret)
  library(dplyr)

  library(tidymodels) # mata-engines for model training
  library(corrplot)   # to generate correlation matrix
  library(ggplot2)    # for plots and graphics
  library(plotmo)     # to generate trace plot 
  library(ggrepel)    # for plotting functions

  library(pls)
```

```{r load data}
  train <- read.csv("../housing_training.csv") %>% na.omit()
  test <- read.csv("../housing_test.csv") %>% na.omit()
```

```{r set seed}
  set.seed(21)
```

```{r Extract Predictors & Response Variabels}
  Y_train <- train$Sale_Price
  X_train <- as.matrix(train[, !(names(train) %in% c("Sale_Price"))])
  X_test <- as.matrix(test[, !(names(test) %in% c("Sale_Price"))])
```

```{r Common Lambda Grid}
  lambda_grid <- exp(seq(-5, 10, length = 100))
```

### Part A

```{r Fitting Lasso Regression 10-Folds}
  lasso_cv <- cv.glmnet(X_train, Y_train, alpha = 1, nfolds = 10,
                        lambda = lambda_grid, standardize = TRUE)
```

```{r Obtain Optimal 位}
  best_lambda <- lasso_cv$lambda.min
  lambda_1se <- lasso_cv$lambda.1se
  plot(lasso_cv)
```

```{r Fit Model using Best 位}
  lasso_model <- glmnet(X_train, Y_train, alpha = 1, lambda = best_lambda, standardize = TRUE)
```

```{r #of Selected Predictors}
  selected_predictors <- sum(coef(lasso_model) != 0) - 1  # Exclude intercept
```

```{r Prediction on Testing Data}
  y_pred <- predict(lasso_model, s = best_lambda, newx = X_test)
```

```{r RMSE plot}
  # Compute test error (RMSE)
  y_test <- test$Sale_Price  # Assuming test dataset has actual Sale_Price
  rmse <- sqrt(mean((y_test - y_pred)^2))
  
  data_plot <- data.frame(lambda = log(lambda_grid), RMSE = lasso_cv$cvm)
  
  rmse_plot <-  ggplot(data_plot, aes(x = lambda, y = RMSE)) +
                geom_point(color = "blue", size = 2) +
                geom_line(color = "blue") +
                labs(x = "Regularization Parameter", y = "RMSE (Cross-Validation)") +
                theme_minimal()
  
  print(rmse_plot)
```

```{r}
# Implement manual cross-validation to correctly compute RMSE
  M <- 10
  rmse <- matrix(NA, ncol = 100, nrow = M)
  train_id_list <- createFolds(Y_train, k = M, returnTrain = TRUE)
  
  for (m in 1:M) {
    tsdata <- train[train_id_list[[m]], ]  # Training data for fold m
    vsdata <- train[-train_id_list[[m]], ]  # Validation data for fold m
    
    x1 <- as.matrix(tsdata[, !(names(tsdata) %in% c("Sale_Price"))])
    y1 <- tsdata$Sale_Price
    x2 <- as.matrix(vsdata[, !(names(vsdata) %in% c("Sale_Price"))])
    y2 <- vsdata$Sale_Price
    
    fit <- glmnet(x1, y1, alpha = 1, lambda = lambda_grid)
    pred <- predict(fit, newx = x2, s = lambda_grid)
    
    rmse[m, ] <- apply((y2 - pred)^2, 2, mean) |> sqrt()
  }

# Compute mean RMSE across folds
  cv_rmse <- colMeans(rmse)
  
# Plot RMSE vs. log(lambda)
  data_plot <- data.frame(lambda = log(lambda_grid), RMSE = cv_rmse)
  
  rmse_plot <- ggplot(data_plot, aes(x = lambda, y = RMSE)) +
    geom_point(color = "blue", size = 2) +
    geom_line(color = "blue") +
    labs(x = "Regularization Parameter", y = "RMSE (Cross-Validation)") +
  theme_minimal()

  print(rmse_plot)

# Select best lambda based on manual CV
  best_lambda <- lambda_grid[which.min(cv_rmse)]

# Fit final model using best lambda
  lasso_model <- glmnet(X_train, Y_train, alpha = 1, lambda = best_lambda, standardize = TRUE)

# Count number of selected predictors (non-zero coefficients)
  selected_predictors <- sum(coef(lasso_model) != 0) - 1  # Exclude intercept

# Predict on test data
  y_pred <- predict(lasso_model, s = best_lambda, newx = X_test)

# Compute test error (RMSE)
  y_test <- test$Sale_Price  # Assuming test dataset has actual Sale_Price
  rmse_test <- sqrt(mean((y_test - y_pred)^2))

# Report results
  cat("Optimal lambda:", best_lambda, "\n")
  cat("Number of selected predictors:", selected_predictors, "\n")
  cat("Test RMSE:", rmse_test, "\n")


```

### Part B

```{r Elastic Net Cross Validation}
  enet.fit <- train(
    Sale_Price ~ .,
    data = train,
    method = "glmnet",
    tuneGrid = expand.grid(alpha = seq(0, 1, length = 20), lambda = lambda_grid),
    trControl = trainControl(method = "cv", number = 10)
  )
```

```{r Acquire Tuning Parameters}
  best_alpha <- enet.fit$bestTune$alpha
  best_lambda_elastic <- enet.fit$bestTune$lambda
  lambda_1se_elastic <- max(enet.fit$results$lambda[enet.fit$results$RMSE <= min(enet.fit$results$RMSE) + sd(enet.fit$results$RMSE)])
```

```{r Fit Final Elastic Net Model}
  elastic_model <- glmnet(X_train, Y_train, alpha = best_alpha, lambda = best_lambda_elastic, standardize = TRUE)
```

```{r Predict on Test Data}
  y_pred_elastic <- predict(elastic_model, s = best_lambda_elastic, newx = X_test)
```

```{r Compute on Test Data RMSE}
  rmse_elastic <- sqrt(mean((test$Sale_Price - y_pred_elastic)^2))
```

```{r Report Results}
  cat("Elastic Net Optimal Alpha:", best_alpha, "\n")
  cat("Elastic Net Optimal Lambda:", best_lambda_elastic, "\n")
  cat("Elastic Net Lambda using 1SE rule:", lambda_1se_elastic, "\n")
  cat("Elastic Net Test RMSE:", rmse_elastic, "\n")
```

```{r}
  enet_rmse_plot <- expand.grid(lambda = log(lambda_grid), alpha = seq(0, 1, length = 20))
  enet_rmse_plot$RMSE <- unlist(lapply(seq(0, 1, length = 20), function(a) enet.fit$results$RMSE[enet.fit$results$alpha == a]))
  ggplot(enet_rmse_plot, aes(x = lambda, y = RMSE, color = factor(round(alpha, 2)), group = factor(round(alpha, 2)))) +
    geom_point(size = 2) + geom_line() +
    geom_line() +
    labs(x = "Log(Lambda)", y = "Cross-Validation RMSE", title = "Elastic Net RMSE vs. Lambda", color = "Mixing Percentage") +
    theme_minimal()

```

### Part C

```{r}
# Fit PLS model with cross-validation
  pls_fit <- train(
    Sale_Price ~ .,
    data = train,
    method = "pls",
    trControl = trainControl(method = "cv", number = 10),
    tuneLength = 20  # Allows automatic selection of the optimal number of components
  )

# Get the best number of components
  best_ncomp <- pls_fit$bestTune$ncomp

# Fit final PLS model using the best number of components
  pls_final <- plsr(Sale_Price ~ ., data = train, ncomp = best_ncomp, validation = "CV")

# Predict on test data
  y_pred_pls <- predict(pls_final, newdata = test, ncomp = best_ncomp)

# Compute test error (RMSE)
  rmse_pls <- sqrt(mean((test$Sale_Price - y_pred_pls)^2))

# Report results
  cat("Optimal number of PLS components:", best_ncomp, "\n")
  cat("Test RMSE (PLS):", rmse_pls, "\n")

```

### Part D

```{r}

```

### Part E

```{r}
# Fit Lasso regression using glmnet
lasso_glmnet <- glmnet(X_train, Y_train, alpha = 1, standardize = TRUE)

# Perform cross-validation
lasso_cv_glmnet <- cv.glmnet(X_train, Y_train, alpha = 1, standardize = TRUE, nfolds = 10)

# Get optimal lambda
best_lambda_glmnet <- lasso_cv_glmnet$lambda.min

# Fit final model using best lambda
lasso_model_glmnet <- glmnet(X_train, Y_train, alpha = 1, lambda = best_lambda_glmnet, standardize = TRUE)

# Predict on test data
y_pred_lasso_glmnet <- predict(lasso_model_glmnet, s = best_lambda_glmnet, newx = X_test)

# Compute test error (RMSE)
rmse_lasso_glmnet <- sqrt(mean((test$Sale_Price - y_pred_lasso_glmnet)^2))

# Report results
cat("Lasso (glmnet) Optimal Lambda:", best_lambda_glmnet, "\n")
cat("Lasso (glmnet) Test RMSE:", rmse_lasso_glmnet, "\n")


```

-   The difference between 28,941.35 (glmnet) and 28,916.63 (caret) is minimal. This suggests both models are performing similarly, with no major discrepancies.

-   **Different Optimal Lambda Values**

    -   `glmnet`: **97.65**

    -   `caret`: **127.55**

    -   This difference could be due to **how cross-validation is implemented** in both approaches:

    1.  **Different Cross-Validation Strategies**

    -   `caret` performs **resampling** with `trainControl(method = "cv", number = 10)`.

    -   `cv.glmnet()` in `glmnet` also uses 10-fold CV, but **it automatically selects 位 values differently**.

    2.  **Grid Search Differences**

    -   `glmnet` **automatically generates** a sequence of 位 values.

    -   `caret` **expands a custom grid (`tuneLength = 20`)**, which may **not perfectly match `glmnet`'s default sequence**.

    3.  **Feature Preprocessing**

    -   `caret` applies **internal preprocessing** (e.g., standardization).

    -   `glmnet` has **built-in standardization** but may handle feature scaling differently.
